{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we train a CNN model to classify images from the CIFAR-10 database. \n",
    "\n",
    "The images are colored images and fall into 10 categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Importing libraries\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Libraries for data loading\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is not available\n"
     ]
    }
   ],
   "source": [
    "#Test for CUDA\n",
    "## Cuda uses GPU and can speed up the training process. Using CUDA can increase the speed in which the model\n",
    "## trains\n",
    "train_on_gpu = torch.cuda.is_available()\n",
    "\n",
    "if train_on_gpu:\n",
    "    print(\"CUDA is available\")\n",
    "else:\n",
    "    print(\"CUDA is not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loadind the Data\n",
    "CIFAR is part of pytorch library so we can download the data and split it into train and test dataset. We will create \n",
    "a dataloader for each of these datasets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Number of subprocess to use\n",
    "num_workers = 0\n",
    "\n",
    "#Number samples per batch\n",
    "batch_size = 20\n",
    "\n",
    "#percentage training set\n",
    "valid_size = 0.2\n",
    "\n",
    "# Transform is defined in PyTorch to transform the given data another format.\n",
    "# Some examples are transforming from numpy array to tensor etc.\n",
    "# Transform compose accepts a list in which we can pass multiple transformations. \n",
    "# Refer to this youtube tutorial for more info - https://www.youtube.com/watch?v=X_QOZEko5uE \n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,0.5,0.5), (0.5,0.5,0.5))\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# Getting training and test Dataset\n",
    "train_data = datasets.CIFAR10('data', train=True, download = True, transform = transform)\n",
    "test_data = datasets.CIFAR10('data', train = False, download = True, transform = transform)\n",
    "\n",
    "# Getting indices and performing training split and validation split\n",
    "num_train = len(train_data)\n",
    "indices = list(range(num_train))\n",
    "np.random.shuffle(indices)\n",
    "split = int(np.floor(valid_size * num_train))\n",
    "train_idx, valid_idx = indices[split:], indices[:split]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Why we need dataloaders\n",
    "\n",
    "When the dataset is huge(in millions) loading everything into the model doesnt make sense as we cant fit all of them to the model and compute gradients for all the data points. This is not efficient. To avoid this we divide the entire dataset into batches of defined size called batch size and got through each batch at once and recompute the gradient and update our weights more formally.  \n",
    "\n",
    "More formally, we define epoch, batch_size and iterations. \n",
    "\n",
    "Epoch - one forward/backward pass through the entire training set.\n",
    "batch_size - The number of training samples through one forward and backward pass.\n",
    "iterations - number of passes. Each pass using batch_size number of examples.\n",
    "\n",
    "Eg: If we have 100 samples in two batches, it takes two iterations to complete 1 epoch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining Samplers\n",
    "train_sampler = SubsetRandomSampler(train_idx)\n",
    "valid_sampler = SubsetRandomSampler(valid_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating dataloader\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size = batch_size, sampler = train_sampler, num_workers = num_workers)\n",
    "valid_loader = torch.utils.data.DataLoader(train_data, batch_size = batch_size, sampler = valid_sampler, num_workers = num_workers)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size = batch_size, num_workers = num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specifying the classes\n",
    "classes = ['airplane', 'automobile', 'bird', 'cat', 'deer','dog','frog','horse','ship','truck']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
